{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial Derivatives\n",
    "\n",
    "You should be familiar with differentiation of a functions with a single variable. Partial differentiation is concerned with functions of more than one variable or variables that are vectors or matrices. The basic rules translate across to partial differentiation - including the important and useful product rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st Order Derivates of functions of more than one variable\n",
    "\n",
    "A function such as $f(x) = x^2 + 5x$ has a single indepdent variable, $x$. If we differentiate with respect to $x$, i.e. we take $\\frac{df}{dx}$, we get $\\frac{df}{dx} = 2x + 5$.\n",
    "\n",
    "A function such as $f(x, y) = x^{3}y^{2} + x^{2} + y^{2}$ has two indepedent variables, $x$ and $y$. Since we have two indepedent variables, we can differentiate with respect to either one. When we differeniate with respect to a variable, we treat the other variables as though they are constants. For example:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial x} = 3x^{2}y^{2} + 2x$ and $\\frac{\\partial f}{\\partial x} = 2x^{3}y + 2y$.\n",
    "\n",
    "Note that instead of writing $\\frac{ \\partial f}{ \\partial x} = f_{x}(x, y)$ and  $\\frac{ \\partial f}{\\partial y} = f_{y}(x, y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excercises \n",
    "Find the following:\n",
    "1. $\\frac{ \\partial f}{\\partial x}$ and $\\frac{ \\partial f}{\\partial y}$ of $2x + 3y^{2}$\n",
    "2. $\\frac{ \\partial f}{\\partial x}$ and $\\frac{ \\partial f}{\\partial y}$ of $\\operatorname{ln}(x) + 3y^{2}$\n",
    "3. $\\frac{ \\partial f}{\\partial x}$ and $\\frac{ \\partial f}{\\partial y}$ of $\\operatorname{sin}(xy)$\n",
    "4. $\\frac{ \\partial f}{\\partial x}$ and $\\frac{ \\partial f}{\\partial y}$ of $e^{xy} + 2x + 4$\n",
    "5. $\\frac{ \\partial f}{\\partial x}$ and $\\frac{ \\partial f}{\\partial y}$ and $\\frac{ \\partial f}{\\partial z}$ of $x + 2y + zy$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Calculus\n",
    "\n",
    "Vector calculus has an intimate relationship with partial derivatives. Consider a function that takes a vector of size $n$ called $x$ and computes the dot product with it and a constant vector $b$\n",
    "$$f(x) = b^{T}x$$\n",
    "\n",
    "If we expand this out and label the vector components by their indicies, we get \n",
    "$$f(x) = b_{1}x_{1} + b_{2}x_{2} + \\ldots + b_{n}x_{n}$$\n",
    "\n",
    "Note that for any $x_{i}$, $\\frac{\\partial f}{\\partial x_{i}} = b_{i}$\n",
    "\n",
    "A derivative of $f$ by the vector $x$ as opposed to any component of $x$ will be a vector\n",
    "$$\\nabla_{x}f = \\left[\\frac{\\partial f}{\\partial x_{1}},\\frac{\\partial f}{\\partial x_{2}}, \\ldots ,\\frac{\\partial f}{\\partial x_{n}} \\right]$$\n",
    "However, since we know that $\\frac{\\partial f}{\\partial x_{i}} = b_{i}$, this means that $\\nabla_{x}f = \\left[b_1, b_2, \\ldots, b_n\\right]$. Since $\\left[b_1, b_2, \\ldots, b_n\\right]$ = $b$, this means that $\\nabla_{x}f = b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Calculus\n",
    "\n",
    "Matrix calculus is very similar to vector calculus. To determine the gradient with respect to a matrix is very similar to determining the gradient with respect to a vector. Note, however, that if the result of the function is a vector, then the gradient is a rank 3 tensor. If the output of the function is a scalar, then the gradient is a matrix itself. In this course, we will only consider the scalar case (we use matrix gradients with respect to loss functions that have scalar output), so we will look at such an example now.\n",
    "\n",
    "\n",
    "Suppose $y = \\operatorname{sum}(Ax)$ where $A \\in \\mathbb{R}^{m \\times n} \\text{ and } x \\in \\mathbb{R}^{n}$. Can we compute $\\nabla_{A} y$ by expanding out the expression $\\operatorname{sum}(Ax)$\n",
    "\n",
    "$$Ax = \\begin{bmatrix}\n",
    "a_{11}x_{1} &+& a_{12}x_{2} &+& \\ldots &+& a_{1n}x_{n} \\\\ \n",
    "a_{21}x_{1} &+& a_{22}x_{2} &+& \\ldots &+& a_{2n}x_{n} \\\\\n",
    "\\vdots  &+&  \\vdots  &+&  \\ldots  &+&  \\vdots \\\\ \n",
    "a_{m1}x_{1}  &+&  a_{m2}x_{2}  &+&  \\ldots  &+&  a_{mn}x_{n} \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$y = \\operatorname{sum}(Ax) = a_{11}x_{1} + a_{12}x_{2} + \\ldots + a_{1n}x_{n} + \\ldots + a_{mn}x_{n}$$\n",
    "\n",
    "Now consider $\\frac{\\partial y}{\\partial a_{ij}} = x_{j}$, wrting this in matrix form, we have that $\\frac{\\partial y}{\\partial A} = \\nabla_{A}y = (x \\otimes \\mathbf{1})^{T}$\n",
    "\n",
    "You can use the [Matrix Calculus Calculator](http://www.matrixcalculus.org) to get a better handle this topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
